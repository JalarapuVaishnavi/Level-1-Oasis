{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2307bdba",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487084aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aae11a",
   "metadata": {},
   "source": [
    "dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f110c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('Twitter_Data.csv')\n",
    "dataset2 = pd.read_csv('apps.csv')\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9138e81",
   "metadata": {},
   "source": [
    "Load positive and negative tweets from the NLTK Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1002613",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = dataset1[dataset1['category'] == 'positive']['clean_text'].tolist()\n",
    "negative_tweets = dataset1[dataset1['category'] == 'negative']['clean_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a396ce",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3ec2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # Tokenize, remove stopwords, and perform stemming\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [ps.stem(word.lower()) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313b41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = [(preprocess_tweets(tweet), 'positive') for tweet in positive_tweets]\n",
    "negative_tweets = [(preprocess_tweets(tweet), 'negative') for tweet in negative_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b755a15",
   "metadata": {},
   "source": [
    "Sentiment Analysis Model Development and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0409ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "training_data = positive_tweets[:4000] + negative_tweets[:4000]\n",
    "testing_data = positive_tweets[4000:] + negative_tweets[4000:]\n",
    "\n",
    "# Create feature sets\n",
    "all_words = [word for tweet, sentiment in training_data for word in tweet.split()]\n",
    "word_freq = FreqDist(all_words)\n",
    "common_words = word_freq.most_common(3000)\n",
    "features = [word for word, freq in common_words]\n",
    "\n",
    "def extract_features(tweet):\n",
    "    words = set(tweet)\n",
    "    features_dict = {}\n",
    "    for word in features:\n",
    "        features_dict[word] = (word in words)\n",
    "    return features_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c759bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import LaplaceProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf145ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jalar\\anaconda3\\xyz\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a64bc56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25144\\3302300905.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtraining_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Evaluate the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\xyz\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# Create the P(label) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0mlabel_probdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\xyz\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, bins)\u001b[0m\n\u001b[0;32m    970\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m         \"\"\"\n\u001b[1;32m--> 972\u001b[1;33m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\xyz\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    849\u001b[0m                 \u001b[1;34m\"A %s probability distribution \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"must have at least one bin.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "for word in features:\n",
    "    if word not in word_freq:\n",
    "        word_freq[word] = 0\n",
    "\n",
    "training_features = [(extract_features(tweet), sentiment) for tweet, sentiment in training_data]\n",
    "testing_features = [(extract_features(tweet), sentiment) for tweet, sentiment in testing_data]\n",
    "\n",
    "# Train the Naive Bayes classifier with Laplace smoothing\n",
    "from nltk.classify.util import apply_features\n",
    "training_features = apply_features(extract_features, training_data)\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_features)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = nltk.classify.accuracy(classifier, testing_features)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c525a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
